---
title: "IndicBART"
weight: 201
url: /indic-bart
---

_Coming soon_

IndicBART is a multilingual,  sequence-to-sequence pre-trained model focusing on Indic languages and English. It currently supports 11 Indian languages and is based on the mBART architecture. You can use IndicBART model to build natural language generation applications for Indian languages by finetuning the model with supervised training data for tasks like machine translation, summarization, question generation, etc. Some salient features of the IndicBART are:

- Supported languages: Assamese, Bengali, Gujarati, Hindi, Marathi, Odiya, Punjabi, Kannada, Malayalam, Tamil, Telugu and English. Not all of these languages are supported by mBART50 and mT5.
- The model is much smaller than the mBART and mT5(-base) models, so less computationally expensive for finetuning and decoding.
- Trained on large Indic language corpora (452 million sentences and 9 billion tokens) which also includes Indian English content. 

### Model Repository

You can download the model and find instructions for model finetuning and decoding in this [GitHub Repo](https://github.com/AI4Bharat/indic-bart). 

### Paper

If you use IndicBART, please cite:




### License

The model is available under the MIT License.
