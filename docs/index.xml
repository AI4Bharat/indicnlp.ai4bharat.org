<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>AI4Bharat IndicNLP</title>
    <link>http://localhost/</link>
    <description>Recent content on AI4Bharat IndicNLP</description>
    <generator>Hugo -- gohugo.io</generator><atom:link href="http://localhost/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>IndicCorp</title>
      <link>http://localhost/corpora/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://localhost/corpora/</guid>
      <description>IndicCorp has been developed by discovering and scraping thousands of web sources - primarily news, magazines and books, over a duration of several months.
IndicCorp is one of the largest publicly-available corpora for Indian languages. It has also been used to train our released models which have obtained state-of-the-art performance on many tasks.
Corpus Format The corpus is a single large text file containing one sentence per line. The publicly released version is randomly shuffled, untokenized and deduplicated.</description>
    </item>
    
    <item>
      <title>Samanantar</title>
      <link>http://localhost/samanantar/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://localhost/samanantar/</guid>
      <description>Samanantar is the largest publicly available parallel corpora collection for Indic languages: Assamese, Bengali, Gujarati, Hindi, Kannada, Malayalam, Marathi, Oriya, Punjabi, Tamil, Telugu. The corpus has 49.6M sentence pairs between English to Indian Languages.
Update 04-11-2021 Samanantar v0.3 along with LaBSE scores metadata is available for download. Go to Downloads
Dataset Format The publicly released version is randomly shuffled, untokenized, and deduplicated.
Downloads Benchmarks The testsets used to benchmark IndicTrans can be found here</description>
    </item>
    
    <item>
      <title>Aksharantar</title>
      <link>http://localhost/aksharantar/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://localhost/aksharantar/</guid>
      <description>Aksharantar is transliteration dataset for Indic languages.
More details, model and dataset download coming soon.</description>
    </item>
    
    <item>
      <title>IndicTrans</title>
      <link>http://localhost/indic-trans/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://localhost/indic-trans/</guid>
      <description>-- IndicTrans is a Transformer-4X model trained on samanantar dataset. Two models are available which can translate from Indic to English and English to Indic. The model can perform translations for 11 lanaguages: Assamese, Bengali, Gujarati, Hindi, Kannada, Malayalam, Marathi, Oriya, Punjabi, Tamil, Telugu.
Update 05-06-2021 The Indic-Indic model is now available for download
Update 30-04-2021 The models are now available for download
Download Model  Indic-English model can be downloaded from here English-Indic model can be downloaded from here Indic-Indic can be downloaded from here  Mirror Links  Please use this mirror gdrive link to download the models  Usage The instructions for running inference can be found at IndicTrans GitHub repository</description>
    </item>
    
    <item>
      <title>IndicXlit</title>
      <link>http://localhost/indic-xlit/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://localhost/indic-xlit/</guid>
      <description>IndicXlit is a Transformer model for romanized input to native Indic language script supporting 21 languages from the Indian subcontinent.
More details, model and dataset download coming soon.</description>
    </item>
    
    <item>
      <title>IndicWav2Vec</title>
      <link>http://localhost/indicwav2vec/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://localhost/indicwav2vec/</guid>
      <description>IndicWav2Vec is a multilingual speech model pretrained on 40 Indian langauges. This model represents the largest diversity of Indian languages in the pool of multilingual speech models. We fine-tune this model for downstream ASR for 9 languages and obtain state-of-the-art results on 3 public benchmarks, namely MUCS, MSR and OpenSLR.
As part of IndicWav2Vec we create largest publicly available corpora for 40 languages from 4 different language families. We also trained state-of-the-art ASR models for 9 Indian languages.</description>
    </item>
    
    <item>
      <title>IndicBERT</title>
      <link>http://localhost/indic-bert/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://localhost/indic-bert/</guid>
      <description>IndicBERT is a multilingual ALBERT model trained on large-scale corpora, covering 12 major Indian languages: Assamese, Bengali, English, Gujarati, Hindi, Kannada, Malayalam, Marathi, Oriya, Punjabi, Tamil, Telugu. IndicBERT has much less parameters than other public models like mBERT and XLM-R while it still manages to give state of the art performance on several tasks.
Download Model The model can be downloaded here. Both tf checkpoints and pytorch binaries are included in the archive.</description>
    </item>
    
    <item>
      <title>IndicBART</title>
      <link>http://localhost/indic-bart/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://localhost/indic-bart/</guid>
      <description>IndicBART is a multilingual, sequence-to-sequence pre-trained model focusing on Indic languages and English. It currently supports 11 Indian languages and is based on the mBART architecture. You can use IndicBART model to build natural language generation applications for Indian languages by finetuning the model with supervised training data for tasks like machine translation, summarization, question generation, etc. Some salient features of the IndicBART are:
 Supported languages: Assamese, Bengali, Gujarati, Hindi, Marathi, Odiya, Punjabi, Kannada, Malayalam, Tamil, Telugu and English.</description>
    </item>
    
    <item>
      <title>IndicGLUE</title>
      <link>http://localhost/indic-glue/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://localhost/indic-glue/</guid>
      <description>To thoroughly evaluate language models on Indian languages, we need a robust NLU benchmark consisting of a wide variety of tasks and covering all the Indian languages. IndicGLUE is a natural language understanding benchmark that we propose. It consists of 6 tasks which we describe in the next section.
In addition, we also compile a list of additional evaluations which comprises of tasks based on publicly-available datasets.
Downloads    Dataset Download Link     Soham News Article Classification link   iNLTK Headline Classification link   BBC News Article Classification link   AI4Bharat Wikipedia Section Titles link   AI4Bharat Cloze-style Question Answering link   AI4Bharat Winnograd Natural Language Inference link   AI4Bharat Choice of Plausible Alternatives link   WikiAnnNER link   CVIT-MKB Cross-lingual Sentence Retrieval link   IITP Movie Reviews Sentiment link   IITP Product Reviews link   ACTSA Sentiment Classifcation link   MIDAS Discourse link   Amrita Paraphrase link (need to request)    The code to run evaluations on the above dataset is provided in the IndicBERT repo.</description>
    </item>
    
    <item>
      <title>IndicNLG Suite</title>
      <link>http://localhost/indicnlg-suite/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://localhost/indicnlg-suite/</guid>
      <description>IndicNLG suite is a collection of datasets for benchmarking Natural Language Generation (NLG) for 11 Indic languages spanning five diverse NLG tasks. The datasets were created using a combination of crawling websites, machine translation, n-gram count and regular expression based cleaning . Overall, the suite contains about 8.5M examples across all languages and tasks and is the largest multilingual NLG dataset to date as well as the first of its kind for Indic languages.</description>
    </item>
    
    <item>
      <title>IndicFT</title>
      <link>http://localhost/indicft/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://localhost/indicft/</guid>
      <description>fastText is a subword-aware word embedding model. It is particularly well-suited for Indian languages due to their highly agglutinative morphology. We train fastText models on our IndicNLP Corpora and evaluate them on a set of tasks to measure its performance.
Our fastText models are available for 11 Indian languages: Assamese, Bengali, English, Gujarati, Hindi, Kannada, Malayalam, Marathi, Oriya, Punjabi, Tamil, Telugu.
Usage To use our fastText models, first download them. Next, install the fastText library:</description>
    </item>
    
    <item>
      <title>Publications</title>
      <link>http://localhost/publications/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://localhost/publications/</guid>
      <description>Aman Kumar, Himani Shrotriya, Prachi Sahu, Raj Dabre, Ratish Puduppully, Anoop Kunchukuttan, Amogh Mishra, Mitesh M. Khapra, Pratyush Kumar. 2022. IndicNLG Suite: Multilingual Datasets for Diverse NLG Tasks in Indic Languages. arXiv preprint arXiv:2203.05437. pdf Raj Dabre, Himani Shrotriya, Anoop Kunchukuttan, Ratish Puduppully, Mitesh M. Khapra, Pratyush Kumar. 2022. IndicBART: A Pre-trained Model for Natural Language Generation of Indic Languages. Findings of the ACL. pdf Gowtham Ramesh, Sumanth Doddapaneni, Aravinth Bheemaraj, Mayank Jobanputra, Raghavan AK, Ajitesh Sharma, Sujit Sahoo, Harshita Diddee, Mahalakshmi J, Divyanshu Kakwani, Navneet Kumar, Aswin Pradeep, Kumar Deepak, Vivek Raghavan, Anoop Kunchukuttan, Pratyush Kumar, Mitesh Shantadevi Khapra.</description>
    </item>
    
    <item>
      <title>About Us</title>
      <link>http://localhost/aboutus/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://localhost/aboutus/</guid>
      <description>Our group focuses on building NLP ecosystem for Indian languages and seeking new models and techniques better suited for Indian languages. Our project has volunteers from IIT Madras, One Fourth Labs, Microsoft Search Technology Center India.
Members       
Contact Us For any queries, feel free to reach us at:
 Anoop Kunchukuttan (anoop.kunchukuttan@gmail.com) Mitesh Khapra (miteshk@cse.iitm.ac.in) Pratyush Kumar (pratyush@cse.iitm.ac.in) Divyanshu Kakwani (divkakwani@gmail.com)  </description>
    </item>
    
    <item>
      <title>Hiring</title>
      <link>http://localhost/hiring/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://localhost/hiring/</guid>
      <description>Our group focuses on building NLP ecosystem for Indian languages and seeking new models and techniques better suited for Indian languages. Our project has volunteers from IIT Madras, One Fourth Labs, Microsoft Search Technology Center India.
Full-time Roles We are recruiting a team of skilled translators who can translate between English and their native language.The ideal candidate has experience in linguistics, particularly syntax at the university level and enjoys working from home, full time.</description>
    </item>
    
    <item>
      <title>Data Explorer</title>
      <link>http://localhost/explorer/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://localhost/explorer/</guid>
      <description>IndicNLP Catalog To get comprehensive information about Indic NLP resources, check out the detailed IndicNLP Catalog on GitHub.
IndicNLP: The Current State Explore, search and add datasets.
 Search Datasets     table.dataTable.no-footer { border-bottom: 0 !important; } tfoot { display: table-row-group; } input[type=&#34;text&#34;] { width: 100%; box-sizing: border-box; -webkit-box-sizing:border-box; -moz-box-sizing: border-box; }   let dataURL = &#34;https://docs.google.com/spreadsheets/d/e/2PACX-1vQGyB-QInM69IoR2nP6pJ_Uc0tA0fRxb1NvDe1F1GvBd7UT9lYW06-DuTjaKTYzmuHbAEPaQR5nhhCb/pub?gid=0&amp;single=true&amp;output=csv&#34;; let encodedURL = encodeURIComponent(dataURL) let proxyURL = `https://api.allorigins.win/raw?url=${encodedURL}`; result = Papa.</description>
    </item>
    
    <item>
      <title>Add a Dataset</title>
      <link>http://localhost/adddataset/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://localhost/adddataset/</guid>
      <description>If you have an Indian language NLP resource that you would want the community to know more about, please help us expand the IndicNLP Data Explorer and Catalog. You can add information about the resources either using the simple form below or head over to the underlying Github catalog and raise a pull request/issue.
Form Loadingâ€¦ </description>
    </item>
    
    <item>
      <title>IndicNLP</title>
      <link>http://localhost/home/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://localhost/home/</guid>
      <description>We are working towards building a better ecosystem for Indian languages. To this end, we are building various resources and models for Indian languages:
 IndicCorp: A lot of NLP models require a large amount of training data, which most of the Indian languages lack. In this project, we develop a large-scale Indic corpora by intesively crawling the web. The corpora that we build has a total of 8.9 billion tokens and covers 12 major Indian languages - making it the largest public corpus for most of the Indian languages.</description>
    </item>
    
    <item>
      <title>Resources</title>
      <link>http://localhost/resources/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://localhost/resources/</guid>
      <description>Edit on Github  Please suggest any other resources you may be aware of. Raise a pull request or an issue to add more resources to the catalog. Put the proposed entry in the following format:
[Wikipedia Dumps](https://dumps.wikimedia.org/)
Add a small, informative description of the dataset and provide links to any paper/article/site documenting the resource. Mention your name too. We would like to acknowlege your contribution to building this catalog in the CONTRIBUTORS list.</description>
    </item>
    
  </channel>
</rss>
